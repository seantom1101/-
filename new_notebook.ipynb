{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall datasets modelscope -y\n",
        "!pip install \"datasets>=3.0.0,<4.0.0\" modelscope[datasets]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7U_a3w-TeT6V",
        "outputId": "73c48234-4926-4124-a294-ef3a9222daf2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "\u001b[33mWARNING: Skipping modelscope as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting datasets<4.0.0,>=3.0.0\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting modelscope[datasets]\n",
            "  Downloading modelscope-1.33.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0.0,>=3.0.0) (6.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (2.5.0)\n",
            "Collecting addict (from modelscope[datasets])\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (25.4.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (0.8.1)\n",
            "Collecting oss2 (from modelscope[datasets])\n",
            "  Downloading oss2-2.19.1.tar.gz (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (1.16.3)\n",
            "Requirement already satisfied: simplejson>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (3.20.2)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.9 in /usr/local/lib/python3.12/dist-packages (from modelscope[datasets]) (2.4.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets<4.0.0,>=3.0.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets<4.0.0,>=3.0.0) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.1->modelscope[datasets]) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.0.0) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<4.0.0,>=3.0.0) (2025.11.12)\n",
            "Collecting crcmod>=1.7 (from oss2->modelscope[datasets])\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->modelscope[datasets])\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->modelscope[datasets])\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->modelscope[datasets])\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<4.0.0,>=3.0.0) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.0.0) (1.22.0)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope[datasets])\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope[datasets]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope[datasets]) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope[datasets]) (2.23)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading modelscope-1.33.0-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: oss2, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=123940 sha256=cda9a0f324cddd1312a1aec7a624c4f179390b344cf2dc30cd7232e09ee7d62a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/aa/ce/a0c9e73f8e4a3b7813b6b0d9dbddfb83028fd416828f27d97b\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535315 sha256=074dfd8b985446d227089c603c777547fd378398f6f30549509cac6923bf5cff\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/60/7c/80d5fdcd6d0e016e4a7ff4a66fd9321b3096ae676d78fe0212\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp312-cp312-linux_x86_64.whl size=31833 sha256=02f13e2ce26b2948251bc5e14e9efd59717e383ff7a8dab40187d221c66fb10c\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/08/0b/caa8b1380122cbfe6a03eaccbec0f63c67e619af4e30ca5e2a\n",
            "Successfully built oss2 aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: crcmod, addict, pycryptodome, jmespath, modelscope, aliyun-python-sdk-core, datasets, aliyun-python-sdk-kms, oss2\n",
            "Successfully installed addict-2.4.0 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 crcmod-1.7 datasets-3.6.0 jmespath-0.10.0 modelscope-1.33.0 oss2-2.19.1 pycryptodome-3.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q timm pillow matplotlib seaborn scikit-learn torch_xla mmcv-full mmcls"
      ],
      "metadata": {
        "id": "EmobU0Jbe6Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e84d51-129a-4e7e-f984-0462019dd0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/607.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/607.9 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.8/648.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\"\"\"\n",
        "Garbage Classification using ModelScope ConvNeXt-Base\n",
        "- Train a new model on validation split and show training process\n",
        "- Evaluate the pre-trained model\n",
        "- Classify images from URL\n",
        "- Optimized for TPU\n",
        "\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from modelscope.msdatasets import MsDataset\n",
        "from modelscope.utils.constant import DownloadMode\n",
        "from modelscope.pipelines import pipeline\n",
        "from modelscope.models import Model\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "k3Q6Fe9WfVJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "use_tpu = False\n",
        "try:\n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    use_tpu = True\n",
        "    print(f\"✓ Using TPU: {device}\")\n",
        "except:\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"✓ Using GPU: {device}\")\n",
        "        print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(f\"✓ Using CPU: {device}\")\n",
        "        print(\"  Note: Training will be slower on CPU\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ydy427qJfYNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"✓ Using GPU: {device}\")\n",
        "        print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "kTpgo2oE4swB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# =============================================================================\n",
        "# 1. LOAD DATASET\n",
        "# =============================================================================\n",
        "print(\"\\n=== Loading Dataset from ModelScope ===\")\n",
        "# Load the garbage265 dataset - using validation split for training\n",
        "ms_train_dataset = MsDataset.load(\n",
        "    'garbage265', namespace='tany0699',\n",
        "    subset_name='default', split='validation',\n",
        "    download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)\n",
        "\n",
        "print(f\"Dataset loaded: {len(ms_train_dataset)} samples\")\n",
        "print(\"Sample data:\", next(iter(ms_train_dataset)))\n",
        "\n",
        "# Split validation set into train and test\n",
        "total_size = len(ms_train_dataset)\n",
        "train_size = int(0.8 * total_size)\n",
        "test_size = total_size - train_size\n",
        "print(f\"\\nSplitting into Train: {train_size}, Test: {test_size}\")"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "wU6vjRT9ffWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# =============================================================================\n",
        "# 2. PREPARE DATASET\n",
        "# =============================================================================\n",
        "class GarbageDataset(Dataset):\n",
        "    def __init__(self, ms_dataset, transform=None):\n",
        "        self.data = list(ms_dataset)\n",
        "        self.transform = transform\n",
        "        print(ms_dataset[0])\n",
        "        # Get unique labels and create mapping\n",
        "        labels = [item['category'] for item in self.data]\n",
        "        self.unique_labels = sorted(list(set(labels)))\n",
        "        self.label_to_idx = {label: idx for idx, label in enumerate(self.unique_labels)}\n",
        "        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}\n",
        "\n",
        "        print(f\"Found {len(self.unique_labels)} classes: {self.unique_labels[:10]}...\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        image = item['image:FILE']\n",
        "        if isinstance(image, str):\n",
        "            image = Image.open(image).convert('RGB')\n",
        "        elif not isinstance(image, Image.Image):\n",
        "            image = Image.open(BytesIO(image)).convert('RGB')\n",
        "\n",
        "        label = self.label_to_idx[item['category']]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RKaSEXRSfkzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create full dataset first to get class info\n",
        "full_dataset = GarbageDataset(ms_train_dataset, transform=train_transform)\n",
        "num_classes = len(full_dataset.unique_labels)\n",
        "class_names = full_dataset.unique_labels\n",
        "\n",
        "# Split into train and test\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "test_dataset.dataset.transform = test_transform\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WNpefALxhKQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# =============================================================================\n",
        "# 3. LOAD PRE-TRAINED MODEL FOR EVALUATION\n",
        "# =============================================================================\n",
        "print(\"\\n=== Loading Pre-trained Model ===\")\n",
        "pretrained_model = Model.from_pretrained('damo/cv_convnext-base_image-classification_garbage')\n",
        "pretrained_classifier = pipeline('image-classification', model=pretrained_model)"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "ZtdUv61qh58X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 4. CREATE AND TRAIN A NEW MODEL\n",
        "# =============================================================================\n",
        "print(\"\\n=== Creating New Model ===\")\n",
        "\n",
        "# Load base ConvNeXt model and modify for our number of classes\n",
        "import timm\n",
        "model = timm.create_model('convnext_base', pretrained=True, num_classes=num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
        "num_epochs = 4 # 10\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "print(f\"\\n=== Training for {num_epochs} epochs ===\")\n",
        "print(f\"Training on {len(train_dataset)} samples\")\n",
        "print(f\"Validating on {len(test_dataset)} samples\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        # For TPU\n",
        "        if 'xla' in str(device):\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100. * correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss = running_loss / len(test_loader)\n",
        "    val_acc = 100. * correct / total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "    print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZsQzTX6OiEYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 5. PLOT TRAINING HISTORY\n",
        "# =============================================================================\n",
        "print(\"\\n=== Plotting Training History ===\")\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss plot\n",
        "ax1.plot(range(1, num_epochs+1), train_losses, 'b-', label='Train Loss', marker='o')\n",
        "ax1.plot(range(1, num_epochs+1), val_losses, 'r-', label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "ax2.plot(range(1, num_epochs+1), train_accs, 'b-', label='Train Acc', marker='o')\n",
        "ax2.plot(range(1, num_epochs+1), val_accs, 'r-', label='Val Acc', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zxWz-AI5jw3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 6. EVALUATE PRE-TRAINED MODEL (4 Main Categories)\n",
        "# =============================================================================\n",
        "print(\"\\n=== Evaluating Pre-trained Model ===\")\n",
        "\n",
        "# Map detailed labels to 4 main categories\n",
        "def get_main_category(label_text):\n",
        "    \"\"\"Extract main category from detailed label\"\"\"\n",
        "    if label_text.startswith('厨余垃圾'):\n",
        "        return 0  # Kitchen waste\n",
        "    elif label_text.startswith('可回收物'):\n",
        "        return 1  # Recyclable\n",
        "    elif label_text.startswith('其他垃圾') or label_text.startswith('其它垃圾'):\n",
        "        return 2  # Other waste\n",
        "    elif label_text.startswith('有害垃圾'):\n",
        "        return 3  # Hazardous waste\n",
        "    else:\n",
        "        return 2  # Default to other waste\n",
        "\n",
        "# Map dataset numeric labels to main categories\n",
        "def map_dataset_label_to_main(label_idx):\n",
        "    \"\"\"Map dataset label index to main category\"\"\"\n",
        "    # Based on the 265 classes order:\n",
        "    # 0-51: 厨余垃圾 (Kitchen waste)\n",
        "    # 52-197: 可回收物 (Recyclable)\n",
        "    # 198-251: 其他垃圾 (Other waste)\n",
        "    # 252-264: 有害垃圾 (Hazardous waste)\n",
        "    if label_idx <= 51:\n",
        "        return 0\n",
        "    elif label_idx <= 197:\n",
        "        return 1\n",
        "    elif label_idx <= 251:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n",
        "\n",
        "main_category_names = ['厨余垃圾', '可回收物', '其他垃圾', '有害垃圾']\n",
        "\n",
        "# Get predictions from pre-trained model\n",
        "pretrained_preds = []\n",
        "true_labels = []\n",
        "\n",
        "for images, labels in tqdm(test_loader, desc='Evaluating Pre-trained'):\n",
        "    # Map true labels to main categories\n",
        "    true_labels.extend([map_dataset_label_to_main(l.item()) for l in labels])\n",
        "\n",
        "    # Convert images back for pipeline\n",
        "    for img_tensor in images:\n",
        "        # Denormalize\n",
        "        img_tensor = img_tensor.cpu()\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        img_tensor = img_tensor * std + mean\n",
        "        img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "\n",
        "        # Convert to PIL\n",
        "        img_pil = transforms.ToPILImage()(img_tensor)\n",
        "\n",
        "        # Predict\n",
        "        result = pretrained_classifier(img_pil)\n",
        "        pred_label_text = result['labels'][0]\n",
        "\n",
        "        # Map prediction to main category\n",
        "        main_cat = get_main_category(pred_label_text)\n",
        "        pretrained_preds.append(main_cat)\n",
        "\n",
        "# Calculate metrics\n",
        "pretrained_acc = accuracy_score(true_labels, pretrained_preds)\n",
        "print(f\"\\nPre-trained Model Accuracy (4 categories): {pretrained_acc*100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix for Pre-trained Model\n",
        "cm_pretrained = confusion_matrix(true_labels, pretrained_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_pretrained, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=main_category_names,\n",
        "            yticklabels=main_category_names)\n",
        "plt.title(f'Confusion Matrix - Pre-trained Model (Acc: {pretrained_acc*100:.2f}%)')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_pretrained.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nPre-trained Model Classification Report:\")\n",
        "print(classification_report(true_labels, pretrained_preds,\n",
        "                          target_names=main_category_names))\n"
      ],
      "metadata": {
        "id": "GAbx4PZHeG2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 8. URL IMAGE CLASSIFICATION FUNCTION\n",
        "# =============================================================================\n",
        "print(\"\\n=== URL Image Classification Ready ===\")\n",
        "\n",
        "def classify_from_url(image_url, use_pretrained=True):\n",
        "    \"\"\"\n",
        "    Classify an image from URL\n",
        "\n",
        "    Args:\n",
        "        image_url: URL of the image\n",
        "        use_pretrained: If True, use pre-trained model, else use newly trained model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Download image\n",
        "        response = requests.get(image_url)\n",
        "        img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "        # Display image\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Input Image\\n{image_url}')\n",
        "        plt.show()\n",
        "\n",
        "        if use_pretrained:\n",
        "            # Use pre-trained model\n",
        "            result = pretrained_classifier(img)\n",
        "            print(\"\\n=== Pre-trained Model Prediction ===\")\n",
        "            for i, (label, score) in enumerate(zip(result['labels'], result['scores'])):\n",
        "                print(f\"{i+1}. {label}: {score:.4f}\")\n",
        "        else:\n",
        "            # Use newly trained model\n",
        "            img_tensor = test_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                outputs = model(img_tensor)\n",
        "                probs = torch.softmax(outputs, dim=1)[0]\n",
        "                top5_prob, top5_idx = torch.topk(probs, min(5, len(class_names)))\n",
        "\n",
        "            print(\"\\n=== Newly Trained Model Prediction ===\")\n",
        "            for i, (idx, prob) in enumerate(zip(top5_idx, top5_prob)):\n",
        "                print(f\"{i+1}. {class_names[idx]}: {prob:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"To classify an image from URL, use:\")\n",
        "print(\"classify_from_url('YOUR_IMAGE_URL', use_pretrained=True)  # Pre-trained model\")\n",
        "print(\"classify_from_url('YOUR_IMAGE_URL', use_pretrained=False) # Newly trained model\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "umcI78duj-1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with a sample URL (you can change this)\n",
        "sample_url = \"https://thumbs.dreamstime.com/z/flattened-coca-cola-can-ground-discarded-disposable-coca-cola-can-crumpled-empty-single-use-fizzy-drink-coke-can-problem-213740333.jpg?ct=jpeg\"\n",
        "print(f\"\\nExample classification with: {sample_url}\")\n",
        "classify_from_url(sample_url, use_pretrained=True)\n",
        "# Example with a sample URL (you can change this)\n",
        "sample_url = \"https://plus.unsplash.com/premium_photo-1724249989963-9286e126af81?q=80&w=1170&auto=format&fit=crop&ixlib=rb-4.1.0&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D\"\n",
        "print(f\"\\nExample classification with: {sample_url}\")\n",
        "classify_from_url(sample_url, use_pretrained=True)"
      ],
      "metadata": {
        "id": "yag1Ls8vLSlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}